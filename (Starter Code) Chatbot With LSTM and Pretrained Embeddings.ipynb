{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "init = False\n",
    "all = False\n",
    "hidden_sizes = {'small':50, 'large':100}\n",
    "hidden_size = hidden_sizes['large']\n",
    "\n",
    "if test:\n",
    "    !pip install -U  pytest\n",
    "if init:\n",
    "    pass\n",
    "    #!pip install torchsummary\n",
    "    #!pip install typing-extensions --upgrade\n",
    "    #!pip install -U torch torchvision torchtext torchdata pytest\n",
    "    #!pip install bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bcolz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbcolz\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_text\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bcolz'"
     ]
    }
   ],
   "source": [
    "# public libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "#from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "from nltk.stem import PorterStemmer\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.nn.utils import weight_norm as wn\n",
    "\n",
    "# my libraries\n",
    "import src.utils\n",
    "import src.data\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "import bcolz\n",
    "import string\n",
    "from src.utils import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "glove_path = \"data\"\n",
    "\n",
    "print(init)\n",
    "if init:\n",
    "    vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.{hidden_size}.dat', mode='w')\n",
    "    with open(f'{glove_path}/glove.6B.{hidden_size}d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vectors.append(vect)\n",
    "\n",
    "    vectors = bcolz.carray(vectors[1:].reshape((400000, 100)), rootdir=f'{glove_path}/6B.{hidden_size}.dat', mode='w')\n",
    "    vectors.flush()\n",
    "    pickle.dump(words, open(f'{glove_path}/6B.{hidden_size}_words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(f'{glove_path}/6B.{hidden_size}_idx.pkl', 'wb'))\n",
    "#\n",
    "vectors = bcolz.open(f'{glove_path}/6B.{hidden_size}.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/6B.{hidden_size}_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/6B.{hidden_size}_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.corpus import brown\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# #from ntlk.stem import *\n",
    "\n",
    "# #nltk.download('brown')\n",
    "# #nltk.download('punkt')\n",
    "\n",
    "# # Output, save, and load brown embeddings\n",
    "\n",
    "# #model = gensim.models.Word2Vec(brown.sents())\n",
    "# #model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "\n",
    "\n",
    "# def loadDF(path):\n",
    "#   '''\n",
    "\n",
    "#   You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "#   '''\n",
    "#   return df\n",
    "\n",
    "\n",
    "# def prepare_text(sentence):\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     Our text needs to be cleaned with a tokenizer. This function will perform that task.\n",
    "#     https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "\n",
    "# def train_test_split(SRC, TRG):\n",
    "\n",
    "#     '''\n",
    "#     Input: SRC, our list of questions from the dataset\n",
    "#             TRG, our list of responses from the dataset\n",
    "\n",
    "#     Output: Training and test datasets for SRC & TRG\n",
    "\n",
    "#     '''\n",
    "#     share = 0.2\n",
    "#     src_split = len(SRC)*share\n",
    "#     trg_split = len(TRG)*share\n",
    "#     SRC_test_dataset = SRC[:scr_split]\n",
    "#     SRC_train_dataset = SRC[scr_split:]\n",
    "#     TRG_test_dataset = SRC[:trg_split]\n",
    "#     TRG_train_dataset = SRC[trg_split:]\n",
    "\n",
    "\n",
    "#     return SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset\n",
    "# print(w2v.wv['the'],len(w2v.wv['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and watch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init:\n",
    "    src.data.squad1_to_csv()\n",
    "    \n",
    "df_train = pd.read_csv('data/train_dataset_squad1.csv')  \n",
    "# df_train = pd.read_csv('data/poc_data.csv')  \n",
    "df_test = pd.read_csv('data/dev_dataset_squad1.csv') \n",
    "# df_test = pd.read_csv('data/poc_data.csv')\n",
    "split = round(len(df_train)*100/(len(df_train)+len(df_test)),1)\n",
    "print(f\"{len(df_train)} training samples and {len(df_test)} have been loaded.\")\n",
    "print(f\"The trainig data makes {split}% of all the data.\")\n",
    "df_train.head()\n",
    "#if all == False:\n",
    "#    df_train = df_train[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_questions = [clean_text(s) for s in df_train[\"question\"].values.tolist()]\n",
    "raw_answers = [clean_text(q[2:-2]) for q in df_train[\"answer\"].values.tolist()]\n",
    "# Source Data\n",
    "plt.title(\"Distribution of lengths of questions\")\n",
    "plt.hist(np.array([len(n) for n in raw_questions]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Data\n",
    "plt.title(\"Distribution of lengths of answers\")\n",
    "plt.hist(np.array([len(n) for n in raw_answers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all == True:\n",
    "    t1 = 25\n",
    "    t2 = 13\n",
    "else:\n",
    "    t1 = 19\n",
    "    t2 = 10\n",
    "index_a = [index for index, row in enumerate(raw_questions) if len(row) < t1]\n",
    "index_b = [index for index, row in enumerate(raw_answers) if len(row) < t2]\n",
    "\n",
    "\n",
    "# index_a = [index for index, row in enumerate(raw_questions) if len(row) > 1]\n",
    "# index_b = [index for index, row in enumerate(raw_answers) if len(row) > 4]\n",
    "\n",
    "\n",
    "index_all=list(set(index_a).intersection(set(index_b)))\n",
    "share = round(((len(df_train)-len(index_all))/len(df_train))*100,1)\n",
    "df_train_filtered = df_train.iloc[index_all]\n",
    "print(f\"We have removed {share}% of the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to repeat this step with the filtered dataframe\n",
    "raw_questions = [clean_text(s) for s in df_train_filtered[\"question\"].values.tolist()]\n",
    "raw_answers = [clean_text(q[2:-2]) for q in df_train_filtered[\"answer\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_prep = src.utils.Vocab(name='prep')\n",
    "vocab_source = src.utils.Vocab(name='source')\n",
    "vocab_target = src.utils.Vocab(name='target')\n",
    "\n",
    "\n",
    "count = 0\n",
    "for q, a in zip(raw_questions, raw_answers):\n",
    "\n",
    "    PAD = \"<PAD>\"\n",
    "    SOS = \"<SOS>\"\n",
    "    EOS = \"<EOS>\"\n",
    "    OUT = \"<OUT>\"\n",
    "    special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<OUT>\"]\n",
    "    for word in special_tokens + q:\n",
    "        vocab_source.indexWord(word)\n",
    "    for word in special_tokens + a:\n",
    "        vocab_target.indexWord(word)\n",
    "\n",
    "print(\"source:\", len(vocab_source.words), \"target:\", len(vocab_target.words), \"common words:\", len(list(set(vocab_source.words.keys()).intersection(set(vocab_target.words.keys())))))\n",
    "\n",
    "questions = src.utils.tokenize_questions(raw_questions, vocab_source)\n",
    "answers = src.utils.tokenize_answers(raw_answers, vocab_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    !python -m pytest -vv srctests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, vocab, glove,dropout=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        weights_matrix = np.zeros((input_size, hidden_size))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(vocab.words):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(hidden_size, ))\n",
    "        print(f\"For {words_found} of {len(vocab.words)} words an entry has been found in glove.\")\n",
    "        \n",
    "        weights_matrix = torch.from_numpy(weights_matrix)\n",
    "        self.embedding.load_state_dict({'weight':weights_matrix})\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, i, h):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state (actually a tuple of hidden state and cell state)\n",
    "        '''\n",
    "        embedding = self.embedding(i)\n",
    "        x,y = h\n",
    "        o, h= self.lstm(embedding, h)\n",
    "        o = self.dropout(o)\n",
    "        \n",
    "        return o, h\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, vocab, glove,dropout):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        \n",
    "        weights_matrix = np.zeros((output_size, hidden_size))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(vocab.words):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(hidden_size, ))\n",
    "        \n",
    "        weights_matrix = torch.from_numpy(weights_matrix)\n",
    "        self.embedding.load_state_dict({'weight':weights_matrix})\n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer  \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, i, h):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the decoder output\n",
    "                h, the hidden state (actually a tuple of hidden state and cell state)\n",
    "        '''\n",
    "\n",
    "        embedding = self.embedding(i)\n",
    "\n",
    "        o, h = self.lstm(embedding, h)\n",
    "\n",
    "        o = self.linear(o)\n",
    "        \n",
    "        o = self.dropout(o)\n",
    "\n",
    "        o = self.softmax(o)\n",
    "\n",
    "        \n",
    "        return o, h\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_source, vocab_target, glove, dropout_E=0, dropout_D=0, teacher_forcing_ratio=1,certain=True):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, vocab_source, glove, dropout_E)\n",
    "        self.decoder = Decoder(hidden_size, output_size, vocab_target, glove,dropout_D)\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.certain = certain\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, start): \n",
    "        '''\n",
    "        Inputs: src, the source vector\n",
    "                trg, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                \n",
    "        '''\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        src.to(device)\n",
    "        trg.to(device)\n",
    "        start.to(device)\n",
    "        \n",
    "            \n",
    "        \n",
    "        # encoder\n",
    "        hidden = (torch.zeros(1,hidden_size).to(device), torch.zeros(1,hidden_size).to(device))\n",
    "        for word in src:\n",
    "            #print(\"word\",word.shape,word)\n",
    "            o, hidden = self.encoder(word.view(-1).to(device), hidden)\n",
    "            x,y = hidden\n",
    "\n",
    "        \n",
    "        # decoder\n",
    "        o = start\n",
    "        #rprint(trg)\n",
    "        \n",
    "        prediction = []\n",
    "        for word in trg:\n",
    "\n",
    "            o, hidden = self.decoder(o.view(-1).to(device), hidden)\n",
    "            x,y = hidden\n",
    "\n",
    "\n",
    "            \n",
    "            if self.training == False:\n",
    "                if self.certain:             \n",
    "                    #print(o)\n",
    "                    o = torch.argmax(o,dim=1)\n",
    "                    #print(o)\n",
    "                #else:\n",
    "                #    o = torch.LongTensor(random.choices(list(range(len(o[0]))), weights=(o[0].detach().cpu().apply_(lambda x: np.exp(x))), k=1))[0]                                                                              \n",
    "                \n",
    "            prediction.append(o)\n",
    "            \n",
    "            if self.training:\n",
    "                o = word if random.random() < self.teacher_forcing_ratio else torch.argmax(o,dim=1)\n",
    "                #o = word if random.random() < self.teacher_forcing_ratio else torch.LongTensor(random.choices(list(range(len(o))), weights=(o.detach().cpu().apply_(lambda x: np.exp(x))), k=1))[0]\n",
    "                                                    \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        prediction = torch.stack(prediction)\n",
    "\n",
    "        prediction = prediction.squeeze()\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "questions = src.utils.tokenize_questions(raw_questions, vocab_source)\n",
    "answers = src.utils.tokenize_answers(raw_answers, vocab_target)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_cuda = torch.cuda.is_available()\n",
    "#is_cuda = False\n",
    "print(f\"Computing on {device}.\")\n",
    "\n",
    "\n",
    "# hyperparams\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "print_each = 30\n",
    "\n",
    "lr = 0.1\n",
    "weight_decay = 0.0\n",
    "penalize_early_eos = 1 # the smaller the higher the penalty, i.e. the less the weight\n",
    "\n",
    "#hidden_size is defined at the very top\n",
    "teacher_forcing_ratio = 0.5 # the higher the teacher_forcing_ratio, the easier it is to learn\n",
    "dropout_E=0.5\n",
    "dropout_D=0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "split = int(0.98*len(questions))\n",
    "batches = len(questions[:split]) // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "input_size = len(vocab_source.words)\n",
    "hidden_size = hidden_size\n",
    "output_size = len(vocab_target.words)\n",
    "model = Seq2Seq(input_size, hidden_size, output_size, vocab_source, vocab_target, glove, dropout_E, dropout_D, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "\n",
    "v = \"submission\"\n",
    "if Path(f\"checkpoints/model_{v}.pt\").is_file():\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/model_{v}.pt\", map_location=torch.device('cpu')))\n",
    "    print(f\"loading from checkpoint: 'checkpoints/model_{v}.pt'\")\n",
    "else:\n",
    "    print(f\"nothing to load at checkpoint: 'checkpoints/model_{v}.pt'\")\n",
    "v=\"submission\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "# training\n",
    "\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "weight = torch.Tensor([1]*output_size).to(device)\n",
    "weight[2]=penalize_early_eos\n",
    "loss_fn = nn.NLLLoss(weight=weight)\n",
    "\n",
    "epoch = 0\n",
    "missed = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for i, (batch_q, batch_a) in enumerate(zip(src.utils.heteroDataLoader(questions[:split],batch_size), src.utils.heteroDataLoader(answers[:split],batch_size))):   \n",
    "        #print(i,\"yes\")\n",
    "        # training loop\n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "\n",
    "        for m, (q, a) in enumerate(zip(batch_q, batch_a)):  \n",
    "            start = next(iter(torch.LongTensor([vocab_target.words[\"<SOS>\"]])))\n",
    "            start.to(device)\n",
    "            a=a.to(device)\n",
    "            q=q.to(device)\n",
    "            output = model(q,a,start)\n",
    "            output.to(device)\n",
    "            model.to(device)\n",
    "            try:\n",
    "                loss = loss_fn(output,a)\n",
    "            except:\n",
    "                print(f\"Loss could not be computed for: {output} with shape {output.shape}.\")\n",
    "                print(f\"Question: {q}\")\n",
    "                print(f\"Answer: {a}\")\n",
    "                print(f\"Old loss will be used with the value of {loss}.\")\n",
    "                loss = loss\n",
    "                \n",
    "            batch_loss += loss\n",
    "\n",
    "        batch_loss = batch_loss/batch_size\n",
    "        batch_loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        train_loss+=batch_loss\n",
    "   \n",
    "        #print(i)\n",
    "        if i % print_each== 0:\n",
    "            print(\"batch:\", f\"{i}/{batches}\")\n",
    "\n",
    "    \n",
    "            \n",
    "        if i % (print_each * 2) == 0:\n",
    "            valid_loss = 0\n",
    "            for n, (batch_q, batch_a) in enumerate(zip(src.utils.heteroDataLoader(questions[split:],batch_size), src.utils.heteroDataLoader(answers[split:],batch_size))):     \n",
    "                # evaluation loop\n",
    "                model.eval()\n",
    "                batch_loss = 0\n",
    "                for q, a in zip(batch_q, batch_a):      \n",
    "                    start = next(iter(torch.LongTensor([vocab_target.words[\"<SOS>\"]])))\n",
    "                    q,a,start = q.to(device),a.to(device),start.to(device)\n",
    "                    assert len(q.shape) ==1, f\"Answer must be 1-dimensional. But {q.shape}\"\n",
    "                    assert len(a.shape) ==1, f\"Answer must be 1-dimensional. But {a.shape}\"\n",
    "                    output = model(q,a,start)\n",
    "                    try:\n",
    "                        loss = loss_fn(output,a)\n",
    "                    except:\n",
    "                        missed = missed +1\n",
    "#                         print(f\"Loss could not be computed for: {output} with shape {output.shape}.\")\n",
    "#                         print(f\"Question: {q}\")\n",
    "#                         print(f\"Answer: {a}\")\n",
    "#                         print(f\"Old loss will be used with the value of {loss}.\")\n",
    "                        loss = loss\n",
    "                    batch_loss += loss\n",
    " \n",
    "                valid_loss += batch_loss / batch_size\n",
    "            valid_loss = round(valid_loss.item() / (len(questions[split:])//batch_size),3)\n",
    "            train_loss = round(train_loss.item() / ((i+1)*5),3)\n",
    "\n",
    "            print(\" \")\n",
    "            \n",
    "            print(\"epoch:\", epoch,\"batch:\", f\"{i}/{batches}\",\"train_loss:\",train_loss, \"valid_loss\", valid_loss)\n",
    "            text = \"\"\n",
    "            for x in q:\n",
    "                text += vocab_source.index[str(x.item())] + \" \"\n",
    "            print(\"question:\",text)\n",
    "            text = \"\"\n",
    "            for x in a:\n",
    "                text += vocab_target.index[str(x.item())] + \" \"\n",
    "            print(\"answer:\", text)\n",
    "            text = \"\"\n",
    "            for x in output:\n",
    "                text += vocab_target.index[str(torch.argmax(x,dim=0).item())] + \" \"\n",
    "            print(\"prediction:\", text)\n",
    "            torch.save(model.state_dict(),f\"checkpoints/model_{v}.pt\")\n",
    "            print(f\"{missed}/{print_each * 2 * batch_size} samples could not be considered.\")\n",
    "            print(\"Saved model.\")\n",
    "            print(\"\")\n",
    "            missed = 0\n",
    "            train_loss = 0\n",
    "            \n",
    "            \n",
    "                  \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = len(vocab_source.words)\n",
    "hidden_size = hidden_size\n",
    "output_size = len(vocab_target.words)\n",
    "teacher_forcing_ratio = 0 # the higher the teacher_forcing_ratio, the easier it is to learn\n",
    "dropout=0.5\n",
    "model = Seq2Seq(input_size, hidden_size, output_size, vocab_source, vocab_target, glove, dropout)\n",
    "\n",
    "v= \"current\"\n",
    "\n",
    "\n",
    "if Path(f\"checkpoints/model_{v}.pt\").is_file():\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/model_{v}.pt\", map_location=torch.device('cpu')))\n",
    "    print(f\"loading from checkpoint: 'checkpoints/model_{v}.pt'\")\n",
    "else:\n",
    "    print(f\"nothing to load at checkpoint: 'checkpoints/model_{v}.pt'\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unravel_index\n",
    "for i,a in enumerate(model.parameters()):\n",
    "    if i ==10:\n",
    "        weights = a.detach().cpu().numpy().flatten()\n",
    "        print(unravel_index(a.cpu().argmax(), a.shape))\n",
    "        \n",
    "plt.hist(weights)\n",
    "print(min(weights),max(weights))\n",
    "\n",
    "from numpy import unravel_index\n",
    "unravel_index(weights.argmax(), weights.shape)\n",
    "print(weights[277])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(model.parameters())[10][2].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.certain=False\n",
    "print(model.certain, model.training)\n",
    "\n",
    "# forward(self, src, trg, start, teacher_forcing_ratio = 0.5):\n",
    "def answer(model, question, answer=None,max_length=15, vocab=vocab_target):\n",
    "    \"\"\"\n",
    "    renders predictions untill the end of sequence or 15...\n",
    "    \"\"\"\n",
    "    try:\n",
    "        max_length = len(answer)\n",
    "    except:\n",
    "        max_length\n",
    "    src = question\n",
    "    dummy = [0]*max_length\n",
    "    trg = torch.LongTensor(dummy)\n",
    "    start = next(iter(torch.LongTensor([vocab.words[\"<SOS>\"]])))\n",
    "\n",
    "    predicted_indices=model(src, trg, start)\n",
    "\n",
    "    predicted_words = [vocab.index[str(a.item())] for a in predicted_indices]\n",
    "    truncated_sequence =[]\n",
    "    for word in predicted_words:\n",
    "        truncated_sequence.append(word)\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "    predicted_answer =  \" \".join(truncated_sequence)\n",
    "    #print(\"question:\", str([vocab_source.index[str(a)] for a in question.tolist()[0]]))\n",
    "    if answer != None:\n",
    "        answer = \" \".join([vocab.index[str(a.item())] for a in answer])\n",
    "        print(\"answer:\",answer)\n",
    "    print(\"predicted_answer:\",predicted_answer)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for i in [12000,35,2000,-10,-5]:\n",
    "    answer(model, questions[i].view(1,-1),answers[i], 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    q = input()\n",
    "    q = clean_text(q)\n",
    "    #hallo mein freund\n",
    "    q=src.utils.tokenize_questions([q], vocab_source)[0]\n",
    "    question = [vocab_source.index[str(a)] for a in q.tolist()]\n",
    "    print(f\"So you are saying: {question} ?\")\n",
    "    a = answer(model, q, max_length=10, vocab=vocab_target)\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
